{
  "metadata" : {
    "name" : "spark-summit-2017-v3",
    "user_save_timestamp" : "1969-12-31T21:00:00.000Z",
    "auto_save_timestamp" : "0022-10-06T21:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [ ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "B9E39EE453B04D58B130E7828C328526"
    },
    "cell_type" : "markdown",
    "source" : "# Spark Summit EU 2017\n\nEiti Kimura, Movile, Brazil\nFlavio Clésio, Movile, Brazil\n\n**Title of the Presentation: PREVENTING REVENUE LEAKAGE AND MONITORING DISTRIBUTED SYSTEMS WITH MACHINE LEARNING **\n\nThursday, October 26 14:40 – 15:10\n\nVenue: Liffey B - Dublin, Ireland \n\nAll of the code presented here was written in Scala language especially for the Spark Summit EU 2017 event."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DFB1F6025E204206865849B701C6F23F"
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.PipelineModel\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.regression.DecisionTreeRegressionModel\nimport org.apache.spark.ml.regression.DecisionTreeRegressor\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\nimport org.apache.spark.ml.tuning.TrainValidationSplitModel\n\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\n\nimport resource._",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.PipelineModel\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.regression.DecisionTreeRegressionModel\nimport org.apache.spark.ml.regression.DecisionTreeRegressor\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\nimport org.apache.spark.ml.tuning.TrainValidationSplitModel\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\nimport resource._\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1,
      "time" : "Took: 1 second 883 milliseconds, at 2017-10-17 8:40"
    } ]
  }, {
    "metadata" : {
      "id" : "04478C53A29A45698A744BC81129A3CD"
    },
    "cell_type" : "markdown",
    "source" : "##Step 1 - Read the dataset\n\nAll of the running processes can be monitored using Spark Interface: http://localhost:4040/jobs/"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "44F9504B37D74EE382D056F71EC5351E"
    },
    "cell_type" : "code",
    "source" : "val ROOT = \"./notebooks/watcher-ai-samples/spark-summit-2017\"\n\n//spark notebook already provide me a sparkSession object, ready to use\nval rawDataFrame = sparkSession.read.load(ROOT + \"/sbs-dataframe\")\n                               .withColumn(\"successful_charges_log\", log($\"successful_charges\"))\n\nval targetColName  = \"successful_charges_log\"\nval featuresColName = \"features_success\"\nval predictionColName = \"prediction_log\"\n\nrawDataFrame\n.select(s\"carrier_id\", s\"successful_charges\", s\"no_credit\", s\"errors\", s\"total_attempts\")\n.show(5)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "+----------+------------------+------------------+------------------+------------------+\n|carrier_id|successful_charges|         no_credit|            errors|    total_attempts|\n+----------+------------------+------------------+------------------+------------------+\n|         1| 33376.07142857143|1750555.9285714286|261810.14285714287| 2045742.142857143|\n|         1| 41120.78571428571|         3468767.5| 533759.8571428572|4043648.1428571427|\n|         1| 43242.57142857143| 5191541.142857143| 810577.9285714286| 6045361.642857143|\n|         1| 58116.72527472527| 6593962.912087912|1303606.3131868131| 7955685.950549451|\n|         1| 59901.08241758242| 7927643.412087912|1805866.0989010988| 9793410.593406593|\n+----------+------------------+------------------+------------------+------------------+\nonly showing top 5 rows\n\nROOT: String = ./notebooks/watcher-ai-samples/spark-summit-2017\nrawDataFrame: org.apache.spark.sql.DataFrame = [carrier_id: int, hour_of_day: int ... 7 more fields]\ntargetColName: String = successful_charges_log\nfeaturesColName: String = features_success\npredictionColName: String = prediction_log\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 3,
      "time" : "Took: 2 seconds 18 milliseconds, at 2017-10-17 9:23"
    } ]
  }, {
    "metadata" : {
      "id" : "34C78C140036453AAF65B200B42C125B"
    },
    "cell_type" : "markdown",
    "source" : "##Step 2 - Split and extract features"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "33E7FA6C30D944799DE74C6470B164BF"
    },
    "cell_type" : "code",
    "source" : "var Array(trainingData, testData) = rawDataFrame.randomSplit(Array(0.8, 0.2), seed = 42)\n\nval featureCols = Array(\"hour_of_day\", \"week_of_month\", \"avg_response_time\",\"no_credit\", \"errors\", \"total_attempts\")\nval vectorAssembler = new VectorAssembler()\n                                .setInputCols(featureCols)\n                                .setOutputCol(featuresColName)\n\nval preparedTrainingData = vectorAssembler.transform(rawDataFrame)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [carrier_id: int, hour_of_day: int ... 7 more fields]\ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [carrier_id: int, hour_of_day: int ... 7 more fields]\nfeatureCols: Array[String] = Array(hour_of_day, week_of_month, avg_response_time, no_credit, errors, total_attempts)\nvectorAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_63972edb7922\npreparedTrainingData: org.apache.spark.sql.DataFrame = [carrier_id: int, hour_of_day: int ... 8 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4,
      "time" : "Took: 1 second 927 milliseconds, at 2017-10-17 9:23"
    } ]
  }, {
    "metadata" : {
      "id" : "964DFCF55A8147A4B950392934BE1414"
    },
    "cell_type" : "markdown",
    "source" : "## Step 3 - Defining Linear Algorithms to Try"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B2B1129527844415B9787A1369EA6359"
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.ml.regression.LinearRegression\n\n// ElasticNet mixing parameter For alpha = 0 the penalty is an L2 penalty (Ridge), for alpha = 1 it is an L1 penalty (Lasso), for 0 < alpha < 1 the penalty is a combination of L1 and L2 (Elastic Net)\nval lrLasso = new LinearRegression()\n  .setMaxIter(100)\n  .setRegParam(0.5) \n  .setElasticNetParam(1) \n  .setLabelCol(targetColName) \n  .setFeaturesCol(featuresColName)\n  .setTol(10)\n  .setPredictionCol(predictionColName)\n\nval lrRidge = new LinearRegression()\n  .setMaxIter(100)\n  .setRegParam(0.5)\n  .setElasticNetParam(0)\n  .setLabelCol(targetColName) \n  .setFeaturesCol(featuresColName)\n  .setTol(10)\n  .setPredictionCol(predictionColName)\n\nval lrElasticNet = new LinearRegression()\n  .setMaxIter(1000)\n  .setRegParam(0.005)\n  .setElasticNetParam(0.2)\n  .setLabelCol(targetColName) \n  .setFeaturesCol(featuresColName)\n  .setTol(10)\n  .setPredictionCol(predictionColName)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.ml.regression.LinearRegression\nlrLasso: org.apache.spark.ml.regression.LinearRegression = linReg_8969bbdb1888\nlrRidge: org.apache.spark.ml.regression.LinearRegression = linReg_f37e5db53579\nlrElasticNet: org.apache.spark.ml.regression.LinearRegression = linReg_ca59154788ba\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 6,
      "time" : "Took: 1 second 85 milliseconds, at 2017-10-17 9:24"
    } ]
  }, {
    "metadata" : {
      "id" : "FABA1F4CDB7247A6960FF0DEB1ED8EBF"
    },
    "cell_type" : "markdown",
    "source" : "##3.1 - Training linear models"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "60FC9F1D27F34DE18A9FCB165F775381"
    },
    "cell_type" : "code",
    "source" : "//fitting the model\nval lassoModel = lrLasso.fit(preparedTrainingData)\nval ridgeModel = lrRidge.fit(preparedTrainingData)\nval elasticNetModel = lrElasticNet.fit(preparedTrainingData)\n\n// summarize the model over the training set and print out some metrics\nval summaryLassoModel = lassoModel.summary\nval summaryRidgeModel = ridgeModel.summary\nval summaryElasticNetModel = elasticNetModel.summary",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "lassoModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_8969bbdb1888\nridgeModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_f37e5db53579\nelasticNetModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_ca59154788ba\nsummaryLassoModel: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@4eab6e6f\nsummaryRidgeModel: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@33b2a86f\nsummaryElasticNetModel: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@2bb6b7ba\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 7,
      "time" : "Took: 4 seconds 808 milliseconds, at 2017-10-17 9:25"
    } ]
  }, {
    "metadata" : {
      "id" : "2FDD41CCF8934A488DD7F1B7FB87C612"
    },
    "cell_type" : "markdown",
    "source" : "##Step 3.2 - Defining the Algorithm to use: Linear Regression with Decision Tree"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B41F0C5BAE944DDC8BC7E3D68BAAA65A"
    },
    "cell_type" : "code",
    "source" : "val decisionTreeAlgorithm = new DecisionTreeRegressor()\n      .setLabelCol(targetColName)\n      .setFeaturesCol(featuresColName)\n      .setPredictionCol(predictionColName)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "decisionTreeAlgorithm: org.apache.spark.ml.regression.DecisionTreeRegressor = dtr_d67d1ed642b5\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 8,
      "time" : "Took: 965 milliseconds, at 2017-10-17 9:25"
    } ]
  }, {
    "metadata" : {
      "id" : "BDAE8309B3E049C0842A7F156AF4C6CF"
    },
    "cell_type" : "markdown",
    "source" : "##Step 4 - Configure the Pipeline execution flow\n  1 - extract features using a fector assembler to mesh all of features of interest togheter\n  \n  2 - mount the pipeline stages with the following sequence: feature extraction, algorithm to train the models"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C310763403DC4FAA965B8F51275D0F7B"
    },
    "cell_type" : "code",
    "source" : "val pipeline = new Pipeline()\n                  .setStages(Array(vectorAssembler, decisionTreeAlgorithm))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "pipeline: org.apache.spark.ml.Pipeline = pipeline_90d505f0a680\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 10,
      "time" : "Took: 1 second 354 milliseconds, at 2017-10-17 9:26"
    } ]
  }, {
    "metadata" : {
      "id" : "744D354D259F4782BECA93D5D42AA30D"
    },
    "cell_type" : "markdown",
    "source" : "##Step 4 - Setup de evaluation step\n\nIn this step will use the hiperparametrization to get the best set of parameters for our model, using the RMSE as the evaluation criteria.\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "F5FC60DD462C4E2A85950CCB41CFCE9B"
    },
    "cell_type" : "code",
    "source" : "val paramGrid = new ParamGridBuilder()\n     .addGrid(decisionTreeAlgorithm.maxDepth, Array(7, 8, 9))\n     .addGrid(decisionTreeAlgorithm.maxBins, (25 to 30).toList)\n     .build()\n\n// Select (prediction, true label) and compute test error.\nval evaluator = new RegressionEvaluator()\n     .setLabelCol(targetColName)\n     .setPredictionCol(predictionColName)\n     .setMetricName(\"rmse\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\nArray({\n\tdtr_d67d1ed642b5-maxBins: 25,\n\tdtr_d67d1ed642b5-maxDepth: 7\n}, {\n\tdtr_d67d1ed642b5-maxBins: 25,\n\tdtr_d67d1ed642b5-maxDepth: 8\n}, {\n\tdtr_d67d1ed642b5-maxBins: 25,\n\tdtr_d67d1ed642b5-maxDepth: 9\n}, {\n\tdtr_d67d1ed642b5-maxBins: 26,\n\tdtr_d67d1ed642b5-maxDepth: 7\n}, {\n\tdtr_d67d1ed642b5-maxBins: 26,\n\tdtr_d67d1ed642b5-maxDepth: 8\n}, {\n\tdtr_d67d1ed642b5-maxBins: 26,\n\tdtr_d67d1ed642b5-maxDepth: 9\n}, {\n\tdtr_d67d1ed642b5-maxBins: 27,\n\tdtr_d67d1ed642b5-maxDepth: 7\n}, {\n\tdtr_d67d1ed642b5-maxBins: 27,\n\tdtr_d67d1ed642b5-maxDepth: 8\n}, {\n\tdtr_d67d1ed642b5-maxBins: 27,\n\tdtr_d67d1ed642b5-maxDepth: 9\n}, {\n\tdtr_d67d1ed642b5-maxBins: 28,\n\tdtr_d67d1ed642b5-maxDepth: 7\n}, {\n\tdtr_d67d1ed642b5-maxBins: 28,\n\tdtr_d67d1ed642b5-maxDepth: 8\n}, {\n\tdtr_d67..."
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 11,
      "time" : "Took: 1 second 251 milliseconds, at 2017-10-17 9:27"
    } ]
  }, {
    "metadata" : {
      "id" : "4AC670B73792468582D2CB09F5874313"
    },
    "cell_type" : "markdown",
    "source" : "Time to tie every thing togheter, the model, pipeline, paramgrid and selected evaluator. \nApache Spark provides an object to deal with all of these elements, train and return the best model we can get.\n\nIntroducing the TrainValidationSplit."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C4D5D51A0A7A46BA84F8AC9E0CCF236C"
    },
    "cell_type" : "code",
    "source" : "val trainValidationSplit = new TrainValidationSplit()\n     .setEstimator(pipeline)\n     .setEvaluator(evaluator)\n     .setEstimatorParamMaps(paramGrid)\n     .setSeed(42)\n     // 80% of the data will be used for training and the remaining 20% for validation.\n     .setTrainRatio(0.8)\n\ntrainValidationSplit.explainParams",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "trainValidationSplit: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_609e19ef32e2\nres14: String =\nestimator: estimator for selection (current: pipeline_90d505f0a680)\nestimatorParamMaps: param maps for the estimator (current: [Lorg.apache.spark.ml.param.ParamMap;@66b9fecf)\nevaluator: evaluator used to select hyper-parameters that maximize the validated metric (current: regEval_2ca2e46c177c)\nseed: random seed (default: -1772833110, current: 42)\ntrainRatio: ratio between training set and validation set (>= 0 && <= 1) (default: 0.75, current: 0.8)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "estimator: estimator for selection (current: pipeline_90d505f0a680)\nestimatorParamMaps: param maps for the estimator (current: [Lorg.apache.spark.ml.param.ParamMap;@66b9fecf)\nevaluator: evaluator used to select hyper-parameters that maximize the validated metric (current: regEval_2ca2e46c177c)\nseed: random seed (default: -1772833110, current: 42)\ntrainRatio: ratio between training set and validation set (&gt;= 0 &amp;&amp; &lt;= 1) (default: 0.75, current: 0.8)"
      },
      "output_type" : "execute_result",
      "execution_count" : 12,
      "time" : "Took: 2 seconds 612 milliseconds, at 2017-10-17 9:29"
    } ]
  }, {
    "metadata" : {
      "id" : "88FE030D411F4AE99F231CDDAB741450"
    },
    "cell_type" : "markdown",
    "source" : "##Step 5 - Time to train the Decision Tree model"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "394A6F29E21448D0BD06607CDC0A141A"
    },
    "cell_type" : "code",
    "source" : "//train a model\nval model = trainValidationSplit.fit(trainingData)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "model: org.apache.spark.ml.tuning.TrainValidationSplitModel = tvs_609e19ef32e2\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 13,
      "time" : "Took: 18 seconds 101 milliseconds, at 2017-10-17 9:30"
    } ]
  }, {
    "metadata" : {
      "id" : "E1976D66220544908CC52E0D20C05F7D"
    },
    "cell_type" : "markdown",
    "source" : "## Step 6 - Evaluate  the results"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab520203358-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"List Unique Values\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "40E5217E1DB248929E990C6CFA24DFEB"
    },
    "cell_type" : "code",
    "source" : "val bestModel = model.bestModel.asInstanceOf[PipelineModel].stages(1).asInstanceOf[DecisionTreeRegressionModel]\nval rmse = model.validationMetrics.min\nprintln(s\"maxDepth: ${bestModel.getMaxDepth}, maxBins: ${bestModel.getMaxBins}, RMSE: $rmse\")\n\n// get all errors for debug purpose\n//model.getEstimatorParamMaps.zip(model.validationMetrics)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "maxDepth: 8, maxBins: 29, RMSE: 0.05863413562165563\nbestModel: org.apache.spark.ml.regression.DecisionTreeRegressionModel = DecisionTreeRegressionModel (uid=dtr_d67d1ed642b5) of depth 8 with 189 nodes\nrmse: Double = 0.05863413562165563\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 14,
      "time" : "Took: 1 second 395 milliseconds, at 2017-10-17 9:30"
    } ]
  }, {
    "metadata" : {
      "id" : "A8B86693A0EC444D8834585BD1CB724A"
    },
    "cell_type" : "markdown",
    "source" : "great! Now we have the best model in our hands!\n\n**For a matter of curiosity, lets calculate the accuracy as well**"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A119697CF8814DC194C4CEC991A6682E"
    },
    "cell_type" : "code",
    "source" : "// prints result of algorithm tested\ndef showMetrics(modelName: String, df: DataFrame): Unit = {\n  val calculateAcc = (expected: Double, predicted: Double) => {\n   val error = (expected - predicted) / expected\n   //println(s\"expected: $expected - pred: $predicted : $error\")\n   if (Math.abs(error) >= 0.15) 0 else 1\n  }\n\n  val calcAccuracyUDF = udf(calculateAcc)\n  val data = df.withColumn(\"result_column\", calcAccuracyUDF(df(\"successful_charges\"), df(\"prediction\")))\n  val total = data.count.toDouble\n\n  //ilter prediction that got right\n  val correct = data.filter(\"result_column = 1\").count.toDouble\n  val accuracy = (correct / total) * 100\n  print(s\"* MODEL: $modelName  ACCURACY: $accuracy%, \")\n}\n\nshowMetrics(\"Linear Regression Lasso\", summaryLassoModel.predictions.withColumn(\"prediction\", exp(predictionColName)))\nprintln(s\"RMSE: ${summaryLassoModel.rootMeanSquaredError}\")\n\nshowMetrics(\"Linear Regression Ridge\", summaryRidgeModel.predictions.withColumn(\"prediction\", exp(predictionColName)))\nprintln(s\"RMSE: ${summaryRidgeModel.rootMeanSquaredError}\")\n\nshowMetrics(\"Linear Regression Elastic Net\", summaryElasticNetModel.predictions.withColumn(\"prediction\", exp(predictionColName)))\nprintln(s\"RMSE: ${summaryElasticNetModel.rootMeanSquaredError}\")\n\nshowMetrics(\"Decision Tree With Linear Regression\", model.transform(testData).withColumn(\"prediction\", exp(predictionColName)))\nprintln(s\"maxDepth: ${bestModel.getMaxDepth}, maxBins: ${bestModel.getMaxBins}, RMSE: $rmse\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "* MODEL: Linear Regression Lasso  ACCURACY: 35.0%, RMSE: 0.3285404294958755\n* MODEL: Linear Regression Ridge  ACCURACY: 87.5%, RMSE: 0.13109519399234304\n* MODEL: Linear Regression Elastic Net  ACCURACY: 35.0%, RMSE: 0.3285404294958755\n* MODEL: Decision Tree With Linear Regression  ACCURACY: 93.33333333333333%, maxDepth: 8, maxBins: 29, RMSE: 0.05863413562165563\nshowMetrics: (modelName: String, df: org.apache.spark.sql.DataFrame)Unit\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 15,
      "time" : "Took: 4 seconds 278 milliseconds, at 2017-10-17 9:31"
    } ]
  }, {
    "metadata" : {
      "id" : "CE3A1F5987C44AE080578B994A94ACB1"
    },
    "cell_type" : "markdown",
    "source" : "## Step 7 - Persist and serialize the chosen model"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D52D2633061E45D58EA46DE69EE56984"
    },
    "cell_type" : "code",
    "source" : "bestModel.write.overwrite.save(ROOT + \"/trained-models-dataframe/success\")",
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 16,
      "time" : "Took: 2 seconds 77 milliseconds, at 2017-10-17 9:31"
    } ]
  } ],
  "nbformat" : 4
}