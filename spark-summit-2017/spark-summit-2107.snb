{
  "metadata" : {
    "name" : "spark-summit-2107",
    "user_save_timestamp" : "1969-12-31T21:00:00.000Z",
    "auto_save_timestamp" : "0022-10-06T21:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [ ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DFB1F6025E204206865849B701C6F23F"
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.PipelineModel\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\n//import org.apache.spark.ml.feature.VectorIndexer\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.regression.DecisionTreeRegressionModel\nimport org.apache.spark.ml.regression.DecisionTreeRegressor\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\nimport org.apache.spark.ml.tuning.TrainValidationSplitModel\n\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.feature.VectorAssembler\n//import org.apache.spark.ml.linalg.Vectors\n\n//import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, TimestampType, DoubleType, DateType}\n//import org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\n\nimport resource._",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.PipelineModel\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.regression.DecisionTreeRegressionModel\nimport org.apache.spark.ml.regression.DecisionTreeRegressor\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\nimport org.apache.spark.ml.tuning.TrainValidationSplitModel\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\nimport resource._\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1,
      "time" : "Took: 1 second 711 milliseconds, at 2017-10-3 16:24"
    } ]
  }, {
    "metadata" : {
      "id" : "04478C53A29A45698A744BC81129A3CD"
    },
    "cell_type" : "markdown",
    "source" : "All of the running processes can be monitored using Spark Interface: http://localhost:4040/jobs/"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "44F9504B37D74EE382D056F71EC5351E"
    },
    "cell_type" : "code",
    "source" : "val ROOT = \"./notebooks/watcher-ai-samples/spark-summit-2017\"\n\n//spark notebook already provide me a sparkSession object, ready to use\nval rawDataFrame = sparkSession.read.load(ROOT + \"/sbs-summarized-dataframe\").cache\n\nval carrierList = List(1)\n\nrawDataFrame\n.select(s\"carrier_id\", s\"successful_charges\", s\"no_credit\", s\"total_attempts\")\n.show(10)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "+----------+------------------+--------------------+--------------------+\n|carrier_id|successful_charges|           no_credit|      total_attempts|\n+----------+------------------+--------------------+--------------------+\n|         1| 33376.07142857143|  1750555.9285714286|   2045742.142857143|\n|         1| 41120.78571428571|           3468767.5|  4043648.1428571427|\n|         1| 43242.57142857143|   5191541.142857143|   6045361.642857143|\n|         1| 58116.72527472527|   6593962.912087912|   7955685.950549451|\n|         1| 59901.08241758242|   7927643.412087912|   9793410.593406593|\n|         1|61559.368131868134|   9414131.554945055|1.1697539807692308E7|\n|         1| 65731.22527472528|1.1097531197802199E7|1.3687370164835164E7|\n|         1| 69519.51098901099|1.2959839983516484E7|1.5799088664835164E7|\n|         1|  70097.2967032967|1.4813599412087914E7|1.7898363164835162E7|\n|         1| 70665.43956043955|1.6607836126373628E7|1.9947964307692304E7|\n+----------+------------------+--------------------+--------------------+\nonly showing top 10 rows\n\nROOT: String = ./notebooks/watcher-ai-samples/spark-summit-2017\nrawDataFrame: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [carrier_id: int, hour_of_day: int ... 6 more fields]\ncarrierList: List[Int] = List(1)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 2,
      "time" : "Took: 8 seconds 327 milliseconds, at 2017-10-3 16:24"
    } ]
  }, {
    "metadata" : {
      "id" : "90BB5E903FD64A49959D6BB51107D890"
    },
    "cell_type" : "markdown",
    "source" : "Time to execute the feature extraction step. It is very simple, we'll just select the features of our interest to use for training the model."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "5ECA86D052F14A968C1A259C642EE40D"
    },
    "cell_type" : "code",
    "source" : "val featureCols = Array(\"hour_of_day\", \"week_of_month\", \"avg_response_time\",\"no_credit\", \"errors\", \"total_attempts\")\n\nval vectorAssembler = new VectorAssembler()\n                                .setInputCols(featureCols)\n                                .setOutputCol(\"features_success\")\n",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "featureCols: Array[String] = Array(hour_of_day, week_of_month, avg_response_time, no_credit, errors, total_attempts)\nvectorAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_f07af78f4442\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 3,
      "time" : "Took: 1 second 698 milliseconds, at 2017-10-3 16:24"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "37A9047733C04EEF83A5523279D813BB"
    },
    "cell_type" : "code",
    "source" : "// Dataframe with all of the  feature columns in  a vector column\nvar dataFrameFeatures = vectorAssembler.transform(rawDataFrame)\n\n// transform and filtering the dataframe\nvar dataFrame = dataFrameFeatures\n                         .withColumn(\"successful_charges_log\", log($\"successful_charges\"))\n                         .filter(\"successful_charges_log is not null\")\n                         .filter(s\"carrier_id = 1\")\n\nval label  = \"successful_charges_log\"\nval features = \"features_success\"\nval predictionColumn = \"successful_charges\"\nval assembler = vectorAssembler",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "dataFrameFeatures: org.apache.spark.sql.DataFrame = [carrier_id: int, hour_of_day: int ... 7 more fields]\ndataFrame: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [carrier_id: int, hour_of_day: int ... 8 more fields]\nlabel: String = successful_charges_log\nfeatures: String = features_success\npredictionColumn: String = successful_charges\nassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_f07af78f4442\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4,
      "time" : "Took: 4 seconds 27 milliseconds, at 2017-10-3 16:24"
    } ]
  }, {
    "metadata" : {
      "id" : "4C23B72CA67E404F834D1957DF484B66"
    },
    "cell_type" : "markdown",
    "source" : "The next step is SPLIT our filtered dataframe"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "4F9CD167EDDA4904B7CBD4E8D58A03A1"
    },
    "cell_type" : "code",
    "source" : "var Array(trainingData, testData) = dataFrame.randomSplit(Array(0.9, 0.1))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [carrier_id: int, hour_of_day: int ... 8 more fields]\ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [carrier_id: int, hour_of_day: int ... 8 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 5,
      "time" : "Took: 1 second 572 milliseconds, at 2017-10-3 16:24"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6EF5A87CF3E94A398A200B227B88897C"
    },
    "cell_type" : "code",
    "source" : "val lr_lasso = new LinearRegression()\n  .setMaxIter(100)\n  .setRegParam(0.5) // Parâmetro de força da regularização\n  .setElasticNetParam(1) // ElasticNet mixing parameter For alpha = 0 the penalty is an L2 penalty (Ridge), for alpha = 1 it is an L1 penalty (Lasso), for 0 < alpha < 1 the penalty is a combination of L1 and L2 (Elastic Net)\n  .setLabelCol(label) \n  .setFeaturesCol(features)\n  .setTol(10) // The convergence tolerance of iterations \n  .setPredictionCol(\"prediction_log\")\n\nval lr_ridge = new LinearRegression()\n  .setMaxIter(100)\n  .setRegParam(0.5) // Parâmetro de força da regularização\n  .setElasticNetParam(0) // ElasticNet mixing parameter For alpha = 0 the penalty is an L2 penalty (Ridge), for alpha = 1 it is an L1 penalty (Lasso), for 0 < alpha < 1 the penalty is a combination of L1 and L2 (Elastic Net)\n  .setLabelCol(label) \n  .setFeaturesCol(features)\n  .setTol(10) // The convergence tolerance of iterations \n  .setPredictionCol(\"prediction_log\")\n\nval lr_elastic_net = new LinearRegression()\n  .setMaxIter(10000)\n  .setRegParam(0.005) // Parâmetro de força da regularização\n  .setElasticNetParam(0.2) // ElasticNet mixing parameter For alpha = 0 the penalty is an L2 penalty (Ridge), for alpha = 1 it is an L1 penalty (Lasso), for 0 < alpha < 1 the penalty is a combination of L1 and L2 (Elastic Net)\n  .setLabelCol(label) \n  .setFeaturesCol(features)\n  .setTol(10) // The convergence tolerance of iterations \n  .setPredictionCol(\"prediction_log\")\n\n//fitting the model\nval lr_lassoModel = lr_lasso.fit(trainingData)\nval lr_ridgeModel = lr_ridge.fit(trainingData)\nval lr_elastic_netModel = lr_elastic_net.fit(trainingData)\n\n// Summarize the model over the training set and print out some metrics\nval trainingSummary_lr_lassoModel = lr_lassoModel.summary\nval trainingSummary_lr_ridgeModel = lr_ridgeModel.summary\nval trainingSummary_lr_elastic_netModel = lr_elastic_netModel.summary",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "lr_lasso: org.apache.spark.ml.regression.LinearRegression = linReg_e1a7790dfebc\nlr_ridge: org.apache.spark.ml.regression.LinearRegression = linReg_1586d9fbf525\nlr_elastic_net: org.apache.spark.ml.regression.LinearRegression = linReg_767a82ad004e\nlr_lassoModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_e1a7790dfebc\nlr_ridgeModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_1586d9fbf525\nlr_elastic_netModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_767a82ad004e\ntrainingSummary_lr_lassoModel: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@2960dca5\ntrainingSummary_lr_ridgeModel: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache...."
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 6,
      "time" : "Took: 6 seconds 756 milliseconds, at 2017-10-3 16:24"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0B9FB95D2C0B43E888FA2C545EE6B21C"
    },
    "cell_type" : "code",
    "source" : "println(s\"Lasso Model RMSE: ${trainingSummary_lr_lassoModel.rootMeanSquaredError}\")\nprintln(s\"Ridge Model RMSE: ${trainingSummary_lr_ridgeModel.rootMeanSquaredError}\")\nprintln(s\"Elastic Net Model RMSE: ${trainingSummary_lr_elastic_netModel.rootMeanSquaredError}\")\ntrainingSummary_lr_elastic_netModel.numInstances\n\n// remove this column, it will be used by vector assembly again in your pipeline model\ndataFrame = dataFrame.drop(s\"features_success\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "Lasso Model RMSE: 0.32493495225845814\nRidge Model RMSE: 0.13068630167289488\nElastic Net Model RMSE: 0.32493495225845814\ndataFrame: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [carrier_id: int, hour_of_day: int ... 7 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 7,
      "time" : "Took: 1 second 375 milliseconds, at 2017-10-3 16:24"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2C780022B3994B658593269C174AC739"
    },
    "cell_type" : "code",
    "source" : "// Criar dois dataframes 1 com o conjunto de features e outro para ser construido no grid das rvores\n// creating label in log format\n// var dataWithLabels = summarizedDataFrame.withColumn(\"successful_charges_log\", log($\"successful_charges\"))\n\n// var dataWithLabelsFiltered = dataWithLabels.filter(\"successful_charges_log is not null\")\n\n// var data = dataWithLabels.filter(s\"carrier_id = 1\")\n\n// var Array(trainingData, testData) = data.randomSplit(Array(0.9, 0.1))",
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 8,
      "time" : "Took: 2 seconds 526 milliseconds, at 2017-10-3 16:24"
    } ]
  }, {
    "metadata" : {
      "id" : "59AD6C5E859A46C78183B5B19E596E43"
    },
    "cell_type" : "markdown",
    "source" : "a set of util functions which build a new evaluator criteria"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2743CA0933A24E8486C63F5DE5876FBD"
    },
    "cell_type" : "code",
    "source" : "def buildEvaluator(label: String, predictionCol: String): RegressionEvaluator = {\n  new RegressionEvaluator()\n    .setLabelCol(label)\n    .setPredictionCol(predictionCol)\n}\n\ndef evaluateR2(df: DataFrame, label: String, predictionCol: String): Double = {\n  val evaluator = buildEvaluator(label, predictionCol)\n  evaluator.setMetricName(\"r2\")\n  evaluator.evaluate(df)\n}\n\ndef evaluateRMSE(df: DataFrame, label: String, predictionCol: String): Double = {\n  val evaluator = buildEvaluator(label, predictionCol)\n  evaluator.setMetricName(\"rmse\")\n  evaluator.evaluate(df)\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "buildEvaluator: (label: String, predictionCol: String)org.apache.spark.ml.evaluation.RegressionEvaluator\nevaluateR2: (df: org.apache.spark.sql.DataFrame, label: String, predictionCol: String)Double\nevaluateRMSE: (df: org.apache.spark.sql.DataFrame, label: String, predictionCol: String)Double\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 9,
      "time" : "Took: 1 second 233 milliseconds, at 2017-10-3 16:24"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DB027F574B3045E48CF73A20DAE62802"
    },
    "cell_type" : "code",
    "source" : "// prints result of algorithm tested\ndef buildStatsMaps(carrier: Double, col: Column, label: String, df: DataFrame, predictionCol: String): Map[String, Any] = {\n  val calculateAcc = (exp: Double, predicted: Double) => {\n    println(s\"expected: $exp => predicted: $predicted\")\n    val error = (exp - predicted) / exp\n    if (error > 0.0) 0 else 1\n  }\n\n  df.select($\"successful_charges\", $\"prediction_successful_charges\").show\n  val calcAccuracyUDF = udf(calculateAcc)\n\n  val rmse = evaluateRMSE(df, label, \"prediction_log\")\n  val r2 = evaluateR2(df, label, \"prediction_log\")\n\n  val data = df.withColumn(\"result_column\", calcAccuracyUDF(col, df(predictionCol)))\n  val total = data.count.toDouble\n  // filter prediction that got right\n  val correct = data.filter(\"result_column = 1\").count.toDouble\n  val accuracy = (correct / total) * 100\n\n  Map(\"rmse\" -> rmse, \"r2\" -> r2, \"accuracy\" -> accuracy)\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "buildStatsMaps: (carrier: Double, col: org.apache.spark.sql.Column, label: String, df: org.apache.spark.sql.DataFrame, predictionCol: String)Map[String,Any]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 27,
      "time" : "Took: 1 second 331 milliseconds, at 2017-10-3 16:53"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab987973994-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "40E5217E1DB248929E990C6CFA24DFEB"
    },
    "cell_type" : "code",
    "source" : "// Production Model\nval trainedModels = carrierList.map { carrier =>\n  val label  = \"successful_charges_log\"\n  val features = \"features_success\"\n  val predictionColumn = \"successful_charges\"\n\n  val data = dataFrame.filter(s\"carrier_id = $carrier\")\n  val Array(trainingData, testData) = data.randomSplit(Array(0.9, 0.1))\n\n  // Train a DecisionTree model.\n  val decisionTree = new DecisionTreeRegressor()\n      .setLabelCol(label)\n      .setFeaturesCol(features)\n      .setPredictionCol(\"prediction_log\")\n\n  val pipeline = new Pipeline().setStages(Array(vectorAssembler, decisionTree))\n\n  val paramGrid = new ParamGridBuilder()\n     .addGrid(decisionTree.maxDepth, Array(7, 8, 9))\n     .addGrid(decisionTree.maxBins, (20 to 30).toList)\n     .build()\n\n  // Select (prediction, true label) and compute test error.\n  val evaluator = new RegressionEvaluator()\n     .setLabelCol(label)\n     .setPredictionCol(\"prediction_log\")\n     .setMetricName(\"rmse\")\n\n\n   val trainValidationSplit = new TrainValidationSplit()\n     .setEstimator(pipeline)\n     .setEvaluator(evaluator)\n     .setEstimatorParamMaps(paramGrid)\n     .setTrainRatio(0.8)\n\n   //train a model\n  val model = trainValidationSplit.fit(trainingData)\n\n  //make predictions\n  val predictions = model.transform(testData)\n  val columnValue = s\"prediction_$predictionColumn\"\n\n  val predictionResult = predictions.withColumn(columnValue, exp($\"prediction_log\"))\n  val statsMap = buildStatsMaps(carrier, predictionResult(columnValue), label, predictionResult, columnValue)\n  \n\n  val bestModel = model.bestModel.asInstanceOf[PipelineModel].stages(1).asInstanceOf[DecisionTreeRegressionModel]\n  println(s\"maxDepth: ${bestModel.getMaxDepth}, maxBins: ${bestModel.getMaxBins}\")\n  println(s\"stats map: ${statsMap}\")\n  (bestModel, carrier, statsMap)\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "+------------------+-----------------------------+\n|successful_charges|prediction_successful_charges|\n+------------------+-----------------------------+\n| 36528.92857142857|           32873.857142857065|\n|  72712.1111111111|            74071.01098901074|\n|61369.071428571435|            64129.50000000001|\n| 57335.28571428571|           58914.428571428565|\n| 61523.57142857142|           64064.428571428536|\n| 81711.77777777777|            78351.66666666667|\n| 75981.07142857142|            80453.15384615383|\n|104519.44444444442|           101566.65384615409|\n| 95460.07142857143|           101566.65384615409|\n|           98741.0|           102296.14285714293|\n| 83723.28571428571|            80630.71428571413|\n| 90228.80952380951|            88762.95238095333|\n+------------------+-----------------------------+\n\nexpected: 32873.857142857065 => predicted: 32873.857142857065\nexpected: 74071.01098901074 => predicted: 74071.01098901074\nexpected: 64129.50000000001 => predicted: 64129.50000000001\nexpected: 58914.428571428565 => predicted: 58914.428571428565\nexpected: 64064.428571428536 => predicted: 64064.428571428536\nexpected: 78351.66666666667 => predicted: 78351.66666666667\nexpected: 80453.15384615383 => predicted: 80453.15384615383\nexpected: 101566.65384615409 => predicted: 101566.65384615409\nexpected: 101566.65384615409 => predicted: 101566.65384615409\nexpected: 102296.14285714293 => predicted: 102296.14285714293\nexpected: 80630.71428571413 => predicted: 80630.71428571413\nexpected: 88762.95238095333 => predicted: 88762.95238095333\nmaxDepth: 9, maxBins: 27\nstats map: Map(rmse -> 0.04865325738857645, r2 -> 0.970571728763207, accuracy -> 100.0)\ntrainedModels: List[(org.apache.spark.ml.regression.DecisionTreeRegressionModel, Int, Map[String,Any])] = List((DecisionTreeRegressionModel (uid=dtr_ce0419f4df6d) of depth 9 with 207 nodes,1,Map(rmse -> 0.04865325738857645, r2 -> 0.970571728763207, accuracy -> 100.0)))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 28,
      "time" : "Took: 29 seconds 960 milliseconds, at 2017-10-3 16:53"
    } ]
  }, {
    "metadata" : {
      "id" : "CE3A1F5987C44AE080578B994A94ACB1"
    },
    "cell_type" : "markdown",
    "source" : "## Evaluating our model"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D52D2633061E45D58EA46DE69EE56984"
    },
    "cell_type" : "code",
    "source" : "trainedModels.foreach{ case (m, c, statsMap) => \n                      m.write.overwrite.save(ROOT + \"/trained-models-dataframe/success-c\" + c)\n                      println(statsMap)}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "Map(rmse -> 0.06139805790870005, r2 -> 0.9620263069018338, accuracy -> 100.0)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 15,
      "time" : "Took: 1 second 988 milliseconds, at 2017-10-3 16:37"
    } ]
  } ],
  "nbformat" : 4
}