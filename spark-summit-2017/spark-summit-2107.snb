{
  "metadata" : {
    "name" : "spark-summit-2107",
    "user_save_timestamp" : "1969-12-31T21:00:00.000Z",
    "auto_save_timestamp" : "0022-10-06T21:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [ ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DFB1F6025E204206865849B701C6F23F"
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.PipelineModel\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.feature.VectorIndexer\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.regression.DecisionTreeRegressionModel\nimport org.apache.spark.ml.regression.DecisionTreeRegressor\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\nimport org.apache.spark.ml.tuning.TrainValidationSplitModel\n\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.linalg.Vectors\n\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, TimestampType, DoubleType, DateType}\nimport org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.functions._\nimport java.util.Calendar\nimport java.util.Date\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\n\nimport resource._\n\nval ROOT = \"/Users/flavio.clesio/Documents/spark-notebook-production/spark-notebook-0.7.0/notebooks/spark-summit-2017\"\n\nval session = SparkSession\n      .builder\n      .appName(\"DecisionTreePipeline\")\n      .getOrCreate()\n\n// You can monitoring all this stuff in: http://localhost:4040/jobs/\n\nval summarizedDataFrame = session.read.load(ROOT + \"/sbs-summarized-dataframe\").cache\n\nval carrierList = List(1)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.PipelineModel\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.feature.VectorIndexer\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.regression.DecisionTreeRegressionModel\nimport org.apache.spark.ml.regression.DecisionTreeRegressor\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\nimport org.apache.spark.ml.tuning.TrainValidationSplitModel\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, TimestampType, DoubleType, DateType}\nimport org.apache.spark.sql.exp..."
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1,
      "time" : "Took: 6 seconds 185 milliseconds, at 2017-9-29 15:20"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "5ECA86D052F14A968C1A259C642EE40D"
    },
    "cell_type" : "code",
    "source" : "val featureCols = Array(\"hour_of_day\", \"week_of_month\", \"avg_response_time\",\"no_credit\", \"errors\", \"total_attempts\")\nval assemblerSucccesful = new VectorAssembler().setInputCols(featureCols).setOutputCol(\"features_success\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "featureCols: Array[String] = Array(hour_of_day, week_of_month, avg_response_time, no_credit, errors, total_attempts)\nassemblerSucccesful: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_e4d6fcd0e9a0\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 2,
      "time" : "Took: 1 second 811 milliseconds, at 2017-9-29 15:20"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "37A9047733C04EEF83A5523279D813BB"
    },
    "cell_type" : "code",
    "source" : "// Dataframe with all of the  feature columns in  a vector column\nvar summarizedDataFrame2 = assemblerSucccesful.transform(summarizedDataFrame)\n\nvar dataWithLabels = summarizedDataFrame2.withColumn(\"successful_charges_log\", log($\"successful_charges\"))\n\nvar dataWithLabelsFiltered = dataWithLabels.filter(\"successful_charges_log is not null\")\n\nvar data = dataWithLabels.filter(s\"carrier_id = 1\")\n\nvar Array(trainingData, testData) = data.randomSplit(Array(0.9, 0.1))\n\nval label  = \"successful_charges_log\"\nval features = \"features_success\"\nval predictionColumn = \"successful_charges\"\nval assembler = assemblerSucccesful",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "summarizedDataFrame2: org.apache.spark.sql.DataFrame = [carrier_id: int, hour_of_day: int ... 7 more fields]\ndataWithLabels: org.apache.spark.sql.DataFrame = [carrier_id: int, hour_of_day: int ... 8 more fields]\ndataWithLabelsFiltered: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [carrier_id: int, hour_of_day: int ... 8 more fields]\ndata: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [carrier_id: int, hour_of_day: int ... 8 more fields]\ntrainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [carrier_id: int, hour_of_day: int ... 8 more fields]\ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [carrier_id: int, hour_of_day: int ... 8 more fields]\nlabel: String = successful_charges_log\nfeatures: String = features_success\npredictionCo..."
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 3,
      "time" : "Took: 3 seconds 120 milliseconds, at 2017-9-29 15:20"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6EF5A87CF3E94A398A200B227B88897C"
    },
    "cell_type" : "code",
    "source" : "val lr_lasso = new LinearRegression()\n  .setMaxIter(100)\n  .setRegParam(0.5) // Parâmetro de força da regularização\n  .setElasticNetParam(1) // ElasticNet mixing parameter For alpha = 0 the penalty is an L2 penalty (Ridge), for alpha = 1 it is an L1 penalty (Lasso), for 0 < alpha < 1 the penalty is a combination of L1 and L2 (Elastic Net)\n  .setLabelCol(label) \n  .setFeaturesCol(features)\n  .setTol(10) // The convergence tolerance of iterations \n  .setPredictionCol(\"prediction_log\")\n\nval lr_ridge = new LinearRegression()\n  .setMaxIter(100)\n  .setRegParam(0.5) // Parâmetro de força da regularização\n  .setElasticNetParam(0) // ElasticNet mixing parameter For alpha = 0 the penalty is an L2 penalty (Ridge), for alpha = 1 it is an L1 penalty (Lasso), for 0 < alpha < 1 the penalty is a combination of L1 and L2 (Elastic Net)\n  .setLabelCol(label) \n  .setFeaturesCol(features)\n  .setTol(10) // The convergence tolerance of iterations \n  .setPredictionCol(\"prediction_log\")\n\nval lr_elastic_net = new LinearRegression()\n  .setMaxIter(10000)\n  .setRegParam(0.005) // Parâmetro de força da regularização\n  .setElasticNetParam(0.2) // ElasticNet mixing parameter For alpha = 0 the penalty is an L2 penalty (Ridge), for alpha = 1 it is an L1 penalty (Lasso), for 0 < alpha < 1 the penalty is a combination of L1 and L2 (Elastic Net)\n  .setLabelCol(label) \n  .setFeaturesCol(features)\n  .setTol(10) // The convergence tolerance of iterations \n  .setPredictionCol(\"prediction_log\")\n\n//fitting the model\nval lr_lassoModel = lr_lasso.fit(trainingData)\nval lr_ridgeModel = lr_ridge.fit(trainingData)\nval lr_elastic_netModel = lr_elastic_net.fit(trainingData)\n\n// Summarize the model over the training set and print out some metrics\nval trainingSummary_lr_lassoModel = lr_lassoModel.summary\nval trainingSummary_lr_ridgeModel = lr_ridgeModel.summary\nval trainingSummary_lr_elastic_netModel = lr_elastic_netModel.summary",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "lr_lasso: org.apache.spark.ml.regression.LinearRegression = linReg_e7dfa8384ed1\nlr_ridge: org.apache.spark.ml.regression.LinearRegression = linReg_fd4f8f49bc22\nlr_elastic_net: org.apache.spark.ml.regression.LinearRegression = linReg_9c661a627de8\nlr_lassoModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_e7dfa8384ed1\nlr_ridgeModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_fd4f8f49bc22\nlr_elastic_netModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_9c661a627de8\ntrainingSummary_lr_lassoModel: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@b88ab17\ntrainingSummary_lr_ridgeModel: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.s..."
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4,
      "time" : "Took: 10 seconds 844 milliseconds, at 2017-9-29 15:20"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0B9FB95D2C0B43E888FA2C545EE6B21C"
    },
    "cell_type" : "code",
    "source" : "println(s\"Lasso Model RMSE: ${trainingSummary_lr_lassoModel.rootMeanSquaredError}\")\nprintln(s\"Ridge Model RMSE: ${trainingSummary_lr_ridgeModel.rootMeanSquaredError}\")\nprintln(s\"Elastic Net Model RMSE: ${trainingSummary_lr_elastic_netModel.rootMeanSquaredError}\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "Lasso Model RMSE: 0.33187866470491684\nRidge Model RMSE: 0.13034342807672586\nElastic Net Model RMSE: 0.33187866470491684\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 5,
      "time" : "Took: 2 seconds 909 milliseconds, at 2017-9-29 15:20"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "684E8C149712475CAB2C910CA2AEEFBF"
    },
    "cell_type" : "code",
    "source" : "trainingSummary_lr_elastic_netModel.numInstances",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res17: Long = 108\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "108"
      },
      "output_type" : "execute_result",
      "execution_count" : 13,
      "time" : "Took: 1 second 834 milliseconds, at 2017-9-29 15:34"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2C780022B3994B658593269C174AC739"
    },
    "cell_type" : "code",
    "source" : "// Criar dois dataframes 1 com o conjunto de features e outro para ser construido no grid das rvores\n// creating label in log format\nvar dataWithLabels = summarizedDataFrame.withColumn(\"successful_charges_log\", log($\"successful_charges\"))\n\nvar dataWithLabelsFiltered = dataWithLabels.filter(\"successful_charges_log is not null\")\n\nvar data = dataWithLabels.filter(s\"carrier_id = 1\")\n\nvar Array(trainingData, testData) = data.randomSplit(Array(0.9, 0.1))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "dataWithLabels: org.apache.spark.sql.DataFrame = [carrier_id: int, hour_of_day: int ... 7 more fields]\ndataWithLabelsFiltered: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [carrier_id: int, hour_of_day: int ... 7 more fields]\ndata: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [carrier_id: int, hour_of_day: int ... 7 more fields]\ntrainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [carrier_id: int, hour_of_day: int ... 7 more fields]\ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [carrier_id: int, hour_of_day: int ... 7 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 6,
      "time" : "Took: 1 second 767 milliseconds, at 2017-9-29 15:25"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "8797D0CC0E284D53A2419A1A2137B940"
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "FC816C0D0BB64AE3A56946F2C7897783"
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "5B8ABA64311A4871BD3BEAB240F613C1"
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "E993E9B36DF54592BD2759AC34704162"
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2743CA0933A24E8486C63F5DE5876FBD"
    },
    "cell_type" : "code",
    "source" : "def buildEvaluator(label: String, predictionCol: String): RegressionEvaluator = {\n  new RegressionEvaluator()\n    .setLabelCol(label)\n    .setPredictionCol(predictionCol)\n}\n\ndef evaluateR2(df: DataFrame, label: String, predictionCol: String): Double = {\n  val evaluator = buildEvaluator(label, predictionCol)\n  evaluator.setMetricName(\"r2\")\n  evaluator.evaluate(df)\n}\n\ndef evaluateRMSE(df: DataFrame, label: String, predictionCol: String): Double = {\n  val evaluator = buildEvaluator(label, predictionCol)\n  evaluator.setMetricName(\"rmse\")\n  evaluator.evaluate(df)\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "buildEvaluator: (label: String, predictionCol: String)org.apache.spark.ml.evaluation.RegressionEvaluator\nevaluateR2: (df: org.apache.spark.sql.DataFrame, label: String, predictionCol: String)Double\nevaluateRMSE: (df: org.apache.spark.sql.DataFrame, label: String, predictionCol: String)Double\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 7,
      "time" : "Took: 1 second 373 milliseconds, at 2017-9-29 15:25"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DB027F574B3045E48CF73A20DAE62802"
    },
    "cell_type" : "code",
    "source" : "// prints result of algorithm tested\ndef buildStatsMaps(carrier: Double, col: Column, label: String, df: DataFrame, predictionCol: String): Map[String, Any] = {\n  val calculateAcc = (exp: Double, predicted: Double) => {\n    val error = (exp - predicted) / exp\n    if (error > 0.2) 0 else 1\n  }\n\n  val calcAccuracyUDF = udf(calculateAcc)\n\n  val rmse = evaluateRMSE(df, label, \"prediction_log\")\n  val r2 = evaluateR2(df, label, \"prediction_log\")\n\n  val data = df.withColumn(\"result_column\", calcAccuracyUDF(col, df(predictionCol)))\n  val total = data.count.toDouble\n  // filter prediction that got right\n  val correct = data.filter(\"result_column = 1\").count.toDouble\n  val accuracy = (correct / total) * 100\n\n  Map(\"rmse\" -> rmse, \"r2\" -> r2, \"accuracy\" -> accuracy)\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "buildStatsMaps: (carrier: Double, col: org.apache.spark.sql.Column, label: String, df: org.apache.spark.sql.DataFrame, predictionCol: String)Map[String,Any]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 8,
      "time" : "Took: 2 seconds 37 milliseconds, at 2017-9-29 15:25"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab987973994-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "40E5217E1DB248929E990C6CFA24DFEB"
    },
    "cell_type" : "code",
    "source" : "// Production Model\nval trainedModels = carrierList.map { c =>\n  val label  = \"successful_charges_log\"\n  val features = \"features_success\"\n  val predictionColumn = \"successful_charges\"\n  val assembler = assemblerSucccesful\n  val data = dataWithLabels.filter(s\"carrier_id = $c\")\n  val Array(trainingData, testData) = data.randomSplit(Array(0.9, 0.1))\n\n  // Train a DecisionTree model.\n  val decisionTree = new DecisionTreeRegressor()\n      .setLabelCol(label)\n      .setFeaturesCol(features)\n      .setPredictionCol(\"prediction_log\")\n\n  val pipeline = new Pipeline().setStages(Array(assemblerSucccesful, decisionTree))\n\n  val paramGrid = new ParamGridBuilder()\n     .addGrid(decisionTree.maxDepth, Array(6, 7, 8))\n     .addGrid(decisionTree.maxBins, (15 to 32).toList)\n     .build()\n\n  // Select (prediction, true label) and compute test error.\n  val evaluator = new RegressionEvaluator()\n     .setLabelCol(label)\n     .setPredictionCol(\"prediction_log\")\n     .setMetricName(\"rmse\")\n\n\n   val trainValidationSplit = new TrainValidationSplit()\n     .setEstimator(pipeline)\n     .setEvaluator(evaluator)\n     .setEstimatorParamMaps(paramGrid)\n     .setTrainRatio(0.8)\n\n   //train a model\n  val model = trainValidationSplit.fit(trainingData)\n\n  //make predictions\n  val predictions = model.transform(testData)\n  val columnValue = s\"prediction_$predictionColumn\"\n\n  val predictionResult = predictions.withColumn(columnValue, exp($\"prediction_log\"))\n  val statsMap = buildStatsMaps(c, predictionResult(columnValue), label, predictionResult, columnValue)\n  \n\n  val bestModel = model.bestModel.asInstanceOf[PipelineModel].stages(1).asInstanceOf[DecisionTreeRegressionModel]\n  println(s\"maxDepth: ${bestModel.getMaxDepth}, maxBins: ${bestModel.getMaxBins}\")\n  (bestModel, c, statsMap)\n}\n",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "maxDepth: 8, maxBins: 32\ntrainedModels: List[(org.apache.spark.ml.regression.DecisionTreeRegressionModel, Int, Map[String,Any])] = List((DecisionTreeRegressionModel (uid=dtr_64926a42d1b1) of depth 8 with 173 nodes,1,Map(rmse -> 0.047478261007086534, r2 -> 0.949548894011031, accuracy -> 100.0)))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 9,
      "time" : "Took: 59 seconds 581 milliseconds, at 2017-9-29 15:27"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "9F8E5C2F3BAF460087F4C32C4E03F8D6"
    },
    "cell_type" : "code",
    "source" : "//(trainedModels(0)._1).toDebugString",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "CE3A1F5987C44AE080578B994A94ACB1"
    },
    "cell_type" : "markdown",
    "source" : "## Evaluating our model\n\nWe can now evaluate our model, simply print the stats already computed:\n<img src=\"http://127.0.0.1:8080/images/step4.png\" width=\"80%\" />"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "D52D2633061E45D58EA46DE69EE56984"
    },
    "cell_type" : "code",
    "source" : "trainedModels.foreach{ case (m, c, statsMap) => \n                      m.write.overwrite.save(ROOT + \"/trained-models-dataframe/success-c\" + c)\n                      println(statsMap)}",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}