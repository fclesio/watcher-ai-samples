{
  "metadata" : {
    "name" : "SP-data-meetup-watcher-ai-notebook",
    "user_save_timestamp" : "1969-12-31T21:00:00.000Z",
    "auto_save_timestamp" : "1969-12-31T21:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [ ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "5FAEE5846C7F4A73806A837101DC993F"
    },
    "cell_type" : "markdown",
    "source" : "# SP Big Data Meetup -   Monitoring High Performance Platforms with Machine Learning\n\n### luciano.sabenca@movile.com / flavio.clesio@movile.com\n\n\nThis notebook shows how to train a decision tree regression model with Apache Spark e MLLib. We will load a sample data, filter it, train the model and calculate its prediction.\n\n\nHere is the our flow. We will take the following steps to train and evaluate our model:\n<img src=\"http://127.0.0.1:8080/images/data-flow.png\" width=\"80%\" />"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6AA87E8B4B09432B850B531955F85D40"
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.PipelineModel\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.feature.VectorIndexer\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.regression.DecisionTreeRegressionModel\nimport org.apache.spark.ml.regression.DecisionTreeRegressor\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\nimport org.apache.spark.ml.tuning.TrainValidationSplitModel\n\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, TimestampType, DoubleType, DateType}\nimport org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.functions._\nimport java.util.Calendar\nimport java.util.Date\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\n\nimport resource._\n\nval ROOT = \"/Users/lucianosabenca/Movile/presentations/see-2017\"\n\nval session = SparkSession\n      .builder\n      .appName(\"DecisionTreePipeline\")\n      .getOrCreate()",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.PipelineModel\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.feature.VectorIndexer\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.regression.DecisionTreeRegressionModel\nimport org.apache.spark.ml.regression.DecisionTreeRegressor\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\nimport org.apache.spark.ml.tuning.TrainValidationSplitModel\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, TimestampType, DoubleType, DateType}\nimport org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.functions._\nimport java.util.Calendar\nimport java.util.Date\nimport org.apache.spark.mllib.tree.model.Dec..."
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1,
      "time" : "Took: 1 second 44 milliseconds, at 2017-8-15 19:7"
    } ]
  }, {
    "metadata" : {
      "id" : "FBA75B4E56254A2796BE9AE38E5C578E"
    },
    "cell_type" : "markdown",
    "source" : "### A Little introduction to Apache Spark\n\nApache Spark is a very powerfull and big framework to distributed computing in Large Cluster. Here we a using this current machine to run a single-node Apache Spark cluster.\nSpark has several modules, such as: \n - SQL and Dataframes\n - Streaming\n - MLLib\n - GraphX\n - etc...\n \nWe will use two modules: SQL and Dataframes and MLLib.\nThe main abstraction used by Spark's Dataframes Modules is called **Dataframe**!\nA dataframe is pretty similar to a table in a relational Database. The data is organized in named columns and each entry is represented as a row.\n\n\nThis is a sample of the data that we will read and use:\n\n<img src=\"http://127.0.0.1:8080/data-format.png\" width=\"80%\" />\n\n\n\nLet's define our schema:\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0C83D64F65BF4D6EA23E947E0B2E9E5D"
    },
    "cell_type" : "code",
    "source" : "\nval customSchema = StructType(Array(\n    StructField(\"carrier_id\", IntegerType, true),\n    StructField(\"datepart\", DateType, true),\n    StructField(\"hour_of_day\", IntegerType, true),\n    StructField(\"avg_response_time\", DoubleType, true),\n    StructField(\"first_attempt\", TimestampType, true),\n    StructField(\"last_attempt\", DoubleType, true),\n    StructField(\"successful_charges\", DoubleType, true),\n    StructField(\"no_credit\", DoubleType, true),\n    StructField(\"errors\", DoubleType, true),\n    StructField(\"total_attempts\", DoubleType, true)\n))\n\nval df = session.read\n        .format(\"org.apache.spark.csv\")\n        .option(\"header\", \"true\") //reading the headers\n        .option(\"delimiter\", \",\")\n        .option(\"mode\", \"DROPMALFORMED\")\n        .schema(customSchema)\n        .csv(ROOT + \"/data-sample.csv\");\n\n//df.printSchema()\ndf.count()\ndf.show(5)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "+----------+----------+-----------+-----------------+--------------------+------------+------------------+---------+--------+--------------+\n|carrier_id|  datepart|hour_of_day|avg_response_time|       first_attempt|last_attempt|successful_charges|no_credit|  errors|total_attempts|\n+----------+----------+-----------+-----------------+--------------------+------------+------------------+---------+--------+--------------+\n|         1|2017-04-01|          0|            913.0|2017-04-01 00:00:...|      2017.0|           41542.0|1497226.0|347585.0|     1886353.0|\n|         1|2017-04-01|          1|           1122.0|2017-04-01 01:00:...|      2017.0|            2879.0|1330047.0|297464.0|     1630390.0|\n|         1|2017-04-01|          2|            973.0|2017-04-01 02:00:...|      2017.0|            1541.0|1449338.0|304554.0|     1755433.0|\n|         1|2017-04-01|          3|           2082.0|2017-04-01 03:00:...|      2017.0|           23492.0| 875965.0|238902.0|     1138359.0|\n|         1|2017-04-01|          4|           1504.0|2017-04-01 04:00:...|      2017.0|            6019.0|1226491.0|304908.0|     1537418.0|\n+----------+----------+-----------+-----------------+--------------------+------------+------------------+---------+--------+--------------+\nonly showing top 5 rows\n\ncustomSchema: org.apache.spark.sql.types.StructType = StructType(StructField(carrier_id,IntegerType,true), StructField(datepart,DateType,true), StructField(hour_of_day,IntegerType,true), StructField(avg_response_time,DoubleType,true), StructField(first_attempt,TimestampType,true), StructField(last_attempt,DoubleType,true), StructField(successful_charges,DoubleType,true), StructField(no_credit,DoubleType,true), StructField(errors,DoubleType,true), StructField(total_attempts,DoubleType,true))\ndf: org.apache.spark.sql.DataFrame = [carrier_id: int, datepart: date ... 8 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 2,
      "time" : "Took: 3 seconds 912 milliseconds, at 2017-8-15 19:7"
    } ]
  }, {
    "metadata" : {
      "id" : "434C0E6D06704DDC882DB5FFBD17C0C4"
    },
    "cell_type" : "markdown",
    "source" : "### Creating some usefull fields\n\nWe will need some functions to deal with the date and extract only the Week of Month from a date.\n\nLet's create it and apply the function to create a new DataFrame with a new column, named *week_of_month*\n\n<img src=\"http://127.0.0.1:8080/images/step2.png\" width=\"80%\" />\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "54B9D2456B914E22B3E7567FB1BBBF95"
    },
    "cell_type" : "code",
    "source" : "// function to extract the week of month from a timestamp\nval udfWoM = (dt :Date) => {\n   val cal = Calendar.getInstance()\n   cal.setTime(dt)\n  \n   cal.setMinimalDaysInFirstWeek(1)\n   cal.get(Calendar.WEEK_OF_MONTH).toDouble\n}\n\n// user defined function\nval weekOfMonthUDF = udf(udfWoM)\n",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "udfWoM: java.util.Date => Double = <function1>\nweekOfMonthUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,DoubleType,None)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 3,
      "time" : "Took: 1 second 266 milliseconds, at 2017-8-15 19:7"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "CD2ADB51D4BC43F88F7BE71549AEE7D9"
    },
    "cell_type" : "code",
    "source" : "// apply UDF to extract the week of month from date time field\nval rawData = df.withColumn(\"week_of_month\", weekOfMonthUDF($\"datepart\"))\n\nrawData.show(20)\n",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "+----------+----------+-----------+-----------------+--------------------+------------+------------------+---------+--------+--------------+-------------+\n|carrier_id|  datepart|hour_of_day|avg_response_time|       first_attempt|last_attempt|successful_charges|no_credit|  errors|total_attempts|week_of_month|\n+----------+----------+-----------+-----------------+--------------------+------------+------------------+---------+--------+--------------+-------------+\n|         1|2017-04-01|          0|            913.0|2017-04-01 00:00:...|      2017.0|           41542.0|1497226.0|347585.0|     1886353.0|          1.0|\n|         1|2017-04-01|          1|           1122.0|2017-04-01 01:00:...|      2017.0|            2879.0|1330047.0|297464.0|     1630390.0|          1.0|\n|         1|2017-04-01|          2|            973.0|2017-04-01 02:00:...|      2017.0|            1541.0|1449338.0|304554.0|     1755433.0|          1.0|\n|         1|2017-04-01|          3|           2082.0|2017-04-01 03:00:...|      2017.0|           23492.0| 875965.0|238902.0|     1138359.0|          1.0|\n|         1|2017-04-01|          4|           1504.0|2017-04-01 04:00:...|      2017.0|            6019.0|1226491.0|304908.0|     1537418.0|          1.0|\n|         1|2017-04-01|          5|           1780.0|2017-04-01 05:00:...|      2017.0|             716.0|1068630.0|269114.0|     1338460.0|          1.0|\n|         1|2017-04-01|          6|           1993.0|2017-04-01 06:00:...|      2017.0|            3137.0| 954644.0|236219.0|     1194000.0|          1.0|\n|         1|2017-04-01|          7|           7781.0|2017-04-01 07:00:...|      2017.0|            1230.0| 226581.0| 95203.0|      323014.0|          1.0|\n|         1|2017-04-01|          8|           2628.0|2017-04-01 08:00:...|      2017.0|             405.0| 727292.0|186685.0|      914382.0|          1.0|\n|         1|2017-04-01|          9|           1992.0|2017-04-01 09:00:...|      2017.0|             404.0| 863743.0|309552.0|     1173699.0|          1.0|\n|         1|2017-04-01|         10|           1273.0|2017-04-01 10:00:...|      2017.0|            2831.0|1248144.0|451967.0|     1702942.0|          1.0|\n|         1|2017-04-01|         11|           1380.0|2017-04-01 11:00:...|      2017.0|            1591.0|1001964.0|342310.0|     1345865.0|          1.0|\n|         1|2017-04-01|         12|           1129.0|2017-04-01 12:00:...|      2017.0|            2753.0|1122619.0|375039.0|     1500411.0|          1.0|\n|         1|2017-04-01|         13|           1105.0|2017-04-01 13:00:...|      2017.0|            2510.0|1132162.0|285833.0|     1420505.0|          1.0|\n|         1|2017-04-01|         14|            987.0|2017-04-01 14:00:...|      2017.0|           12368.0|1395526.0|303456.0|     1711350.0|          1.0|\n|         1|2017-04-01|         15|            937.0|2017-04-01 15:00:...|      2017.0|            4079.0|1252094.0|292082.0|     1548255.0|          1.0|\n|         1|2017-04-01|         16|           1003.0|2017-04-01 16:00:...|      2017.0|            4753.0|1413715.0|329549.0|     1748017.0|          1.0|\n|         1|2017-04-01|         17|            935.0|2017-04-01 17:00:...|      2017.0|            3629.0|1483959.0|352642.0|     1840230.0|          1.0|\n|         1|2017-04-01|         18|           1328.0|2017-04-01 18:00:...|      2017.0|            3434.0|1284684.0|283894.0|     1572012.0|          1.0|\n|         1|2017-04-01|         19|           1072.0|2017-04-01 19:00:...|      2017.0|            4557.0|1384854.0|327259.0|     1716670.0|          1.0|\n+----------+----------+-----------+-----------------+--------------------+------------+------------------+---------+--------+--------------+-------------+\nonly showing top 20 rows\n\nrawData: org.apache.spark.sql.DataFrame = [carrier_id: int, datepart: date ... 9 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4,
      "time" : "Took: 1 second 545 milliseconds, at 2017-8-15 19:7"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "90D7D7022AB848B28564E2C3B01C1486"
    },
    "cell_type" : "code",
    "source" : "rawData.groupBy(\"week_of_month\").count().show()",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "+-------------+-----+\n|week_of_month|count|\n+-------------+-----+\n|          1.0|   96|\n|          4.0|  672|\n|          3.0|  672|\n|          2.0|  672|\n|          6.0|   96|\n|          5.0|  672|\n+-------------+-----+\n\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 5,
      "time" : "Took: 2 seconds 204 milliseconds, at 2017-8-15 19:7"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "32F2554FC86F406D833716A4391CA58B"
    },
    "cell_type" : "code",
    "source" : "val avgDataFrame = rawData.groupBy(\"carrier_id\", \"hour_of_day\",\"week_of_month\").agg(avg(\"avg_response_time\").as(\"avg_response_time\"), \n                                                                                        avg(\"successful_charges\").as(\"successful_charges\"), \n                                                                                        avg(\"no_credit\").as(\"no_credit\"), \n                                                                                        avg(\"errors\").as(\"errors\"),\n                                                                                        avg(\"total_attempts\").as(\"total_attempts\"))\n\n\n\navgDataFrame.show(10)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "+----------+-----------+-------------+------------------+------------------+-----------------+-----------------+------------------+\n|carrier_id|hour_of_day|week_of_month| avg_response_time|successful_charges|        no_credit|           errors|    total_attempts|\n+----------+-----------+-------------+------------------+------------------+-----------------+-----------------+------------------+\n|         4|         19|          1.0|            2109.0|            9608.0|         487233.0|          73857.0|          579364.0|\n|         1|         10|          6.0|             845.0|             336.0|        1973116.0|         295121.0|         2268573.0|\n|         2|         13|          3.0| 588.4285714285714|            9403.0|4611578.857142857|80729.14285714286|         4701711.0|\n|         5|          7|          5.0|249.57142857142858|353.42857142857144|1949208.857142857|          52454.0|2002016.2857142857|\n|         2|         17|          5.0|             609.0| 5160.142857142857|4349954.714285715|82985.14285714286|         4438100.0|\n|         5|         20|          1.0|             514.0|            2698.0|        1113997.0|          66236.0|         1182931.0|\n|         1|         18|          6.0|             880.0|            1863.0|        1912694.0|         274198.0|         2188755.0|\n|         2|          1|          6.0|             495.0|            1455.0|        4452571.0|          53203.0|         4507229.0|\n|         2|          7|          5.0| 374.7142857142857| 341.7142857142857|6197190.428571428|74260.57142857143| 6271792.714285715|\n|         2|         15|          5.0| 658.7142857142857| 5454.857142857143|4375501.571428572|70339.42857142857| 4451295.857142857|\n+----------+-----------+-------------+------------------+------------------+-----------------+-----------------+------------------+\nonly showing top 10 rows\n\navgDataFrame: org.apache.spark.sql.DataFrame = [carrier_id: int, hour_of_day: int ... 6 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 6,
      "time" : "Took: 1 second 628 milliseconds, at 2017-8-15 19:7"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D184782F35784E428776DA9A66BE6FC5"
    },
    "cell_type" : "code",
    "source" : "rawData.where(\"carrier_id = 4 AND hour_of_day = 19 and WEEk_of_month = 5\").show()",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "+----------+----------+-----------+-----------------+--------------------+------------+------------------+---------+-------+--------------+-------------+\n|carrier_id|  datepart|hour_of_day|avg_response_time|       first_attempt|last_attempt|successful_charges|no_credit| errors|total_attempts|week_of_month|\n+----------+----------+-----------+-----------------+--------------------+------------+------------------+---------+-------+--------------+-------------+\n|         4|2017-04-23|         19|           2317.0|2017-04-23 19:00:...|      2017.0|             548.0| 497842.0|28588.0|      528027.0|          5.0|\n|         4|2017-04-24|         19|           2445.0|2017-04-24 19:00:...|      2017.0|            1456.0| 475119.0|38734.0|      517114.0|          5.0|\n|         4|2017-04-25|         19|           2251.0|2017-04-25 19:00:...|      2017.0|            1159.0| 511993.0|27604.0|      542295.0|          5.0|\n|         4|2017-04-26|         19|           2434.0|2017-04-26 19:00:...|      2017.0|            2090.0| 488524.0|32813.0|      525274.0|          5.0|\n|         4|2017-04-27|         19|           2248.0|2017-04-27 19:00:...|      2017.0|            1330.0| 520943.0|24058.0|      548116.0|          5.0|\n|         4|2017-04-28|         19|           2274.0|2017-04-28 19:00:...|      2017.0|            1521.0| 441204.0|26021.0|      470437.0|          5.0|\n|         4|2017-04-29|         19|           2192.0|2017-04-29 19:00:...|      2017.0|            1305.0| 512479.0|27419.0|      542858.0|          5.0|\n+----------+----------+-----------+-----------------+--------------------+------------+------------------+---------+-------+--------------+-------------+\n\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 7,
      "time" : "Took: 1 second 411 milliseconds, at 2017-8-15 19:7"
    } ]
  }, {
    "metadata" : {
      "id" : "EEE69F4EC9DE4FF284DB1E53E8B1AC19"
    },
    "cell_type" : "markdown",
    "source" : "The data now looks like: "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "44B05F8F6A3E4825830D2CF56DFFC425"
    },
    "cell_type" : "code",
    "source" : "avgDataFrame.where(\"carrier_id = 4 AND hour_of_day = 19 and WEEk_of_month = 5\").show()",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "+----------+-----------+-------------+-----------------+------------------+------------------+------------------+-----------------+\n|carrier_id|hour_of_day|week_of_month|avg_response_time|successful_charges|         no_credit|            errors|   total_attempts|\n+----------+-----------+-------------+-----------------+------------------+------------------+------------------+-----------------+\n|         4|         19|          5.0|2308.714285714286| 1344.142857142857|492586.28571428574|29319.571428571428|524874.4285714285|\n+----------+-----------+-------------+-----------------+------------------+------------------+------------------+-----------------+\n\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 8,
      "time" : "Took: 1 second 501 milliseconds, at 2017-8-15 19:7"
    } ]
  }, {
    "metadata" : {
      "id" : "4A8DF72B0D094B7F9452611F8BCC6821"
    },
    "cell_type" : "markdown",
    "source" : "We still need to agreggate summing the values from the past hours until the current hour:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "221A8AE317AF44AD8CF4028C8C0DA1FF"
    },
    "cell_type" : "code",
    "source" : "val carrierList = List(1)\n\n// filtering data by carrier_id\nval listDataFrame = carrierList.map{id =>\n    avgDataFrame.filter(\"carrier_id = \" + id)\n}\n\n\n//means from beginning until current position\nval wSpec = Window.partitionBy(\"week_of_month\").orderBy(\"hour_of_day\").rowsBetween(Long.MinValue, 0)\n\nval summarizedList = listDataFrame.map{dataframe => \n    val df = dataframe.withColumn(\"avg_response_time\", avg($\"avg_response_time\").over(wSpec))\n               .withColumn(\"successful_charges\", sum($\"successful_charges\").over(wSpec))\n               .withColumn(\"no_credit\", sum($\"no_credit\").over(wSpec))\n               .withColumn(\"errors\", sum($\"errors\").over(wSpec))\n               .withColumn(\"total_attempts\", sum($\"total_attempts\").over(wSpec))\n    df //return the dataset                                     \n}\n\nsummarizedList.foreach{_.show(3)}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "+----------+-----------+-------------+------------------+------------------+---------+--------+--------------+\n|carrier_id|hour_of_day|week_of_month| avg_response_time|successful_charges|no_credit|  errors|total_attempts|\n+----------+-----------+-------------+------------------+------------------+---------+--------+--------------+\n|         1|          0|          1.0|             913.0|           41542.0|1497226.0|347585.0|     1886353.0|\n|         1|          1|          1.0|            1017.5|           44421.0|2827273.0|645049.0|     3516743.0|\n|         1|          2|          1.0|1002.6666666666666|           45962.0|4276611.0|949603.0|     5272176.0|\n+----------+-----------+-------------+------------------+------------------+---------+--------+--------------+\nonly showing top 3 rows\n\ncarrierList: List[Int] = List(1)\nlistDataFrame: List[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = List([carrier_id: int, hour_of_day: int ... 6 more fields])\nwSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@691f5832\nsummarizedList: List[org.apache.spark.sql.DataFrame] = List([carrier_id: int, hour_of_day: int ... 6 more fields])\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 9,
      "time" : "Took: 2 seconds 872 milliseconds, at 2017-8-15 19:7"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "529114E421364B6F8E01C63F8681E943"
    },
    "cell_type" : "markdown",
    "source" : "At this point, we have a list for each carrier with only its data. We will create a unique dataframe again with all the data and write it to a disk. This will be our checkpoint!\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D6443F98F59F4010A3227E999EACD3C1"
    },
    "cell_type" : "code",
    "source" : "val summarizedDataFrame = summarizedList.reduce(_ union _)\nsummarizedDataFrame.count\n//write data to disk in parquet format\nsummarizedDataFrame.write.format(\"parquet\").mode(\"overwrite\").save(ROOT + \"/sbs-summarized-dataframe\")\n",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "summarizedDataFrame: org.apache.spark.sql.DataFrame = [carrier_id: int, hour_of_day: int ... 6 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 10,
      "time" : "Took: 5 seconds 213 milliseconds, at 2017-8-15 19:7"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "282390EB58074DAC8DF220D630C356A4"
    },
    "cell_type" : "markdown",
    "source" : "To read again the data from the disk, you just need to run the following command:\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "43366C5EBC0E4DC2B469919FCDE166F3"
    },
    "cell_type" : "code",
    "source" : "val summarizedDataFrame = session.read.load(ROOT + \"/sbs-summarized-dataframe\").cache",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "summarizedDataFrame: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [carrier_id: int, hour_of_day: int ... 6 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 11,
      "time" : "Took: 876 milliseconds, at 2017-8-15 19:7"
    } ]
  }, {
    "metadata" : {
      "id" : "442D73C10DDD4C5488D4582445FCFD0E"
    },
    "cell_type" : "markdown",
    "source" : "### Setting our model's labels and features\n\nWe are now ready to create our model's features and labels! Just before that, let's do a quick remember on our naming: \n- **Features**: Our features are the independent variables. In our linear model, it is multiplied by the trained coeficients.\n- **Label**: Is our dependent variables. Our target is predict its value.\n\nIn this example, we want to predict our **success** number using as input the following features: *hour_of_day*, *week_of_month*, *avg_response_time*, *no_credit*, *errors*, *total_attempts*."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "7699BA1DA2E84B9188C2DA6B16B67177"
    },
    "cell_type" : "code",
    "source" : "// features\nval assemblerSucccesful = new VectorAssembler()\n                      .setInputCols(Array(\"hour_of_day\", \"week_of_month\", \"avg_response_time\",\"no_credit\", \"errors\", \"total_attempts\"))\n                      .setOutputCol(\"features_success\")\n\n\n\n// creating label in log format\nval dataWithLabels = summarizedDataFrame.withColumn(\"successful_charges_log\", log($\"successful_charges\"))\n\n\n\nval dataWithLabelsFiltered = dataWithLabels.filter(\"successful_charges_log is not null\")\n",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "assemblerSucccesful: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_88799945f856\ndataWithLabels: org.apache.spark.sql.DataFrame = [carrier_id: int, hour_of_day: int ... 7 more fields]\ndataWithLabelsFiltered: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [carrier_id: int, hour_of_day: int ... 7 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 12,
      "time" : "Took: 781 milliseconds, at 2017-8-15 19:8"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C9CE881AE4994758884FBA994BB6118E"
    },
    "cell_type" : "markdown",
    "source" : "### Setting our metrics and validators\n\nWe have now to define which metrics will be usefull now! \nWe have defined, after some tests, 3 classical metrics: \n - **RMSE (Root Mean Squared Error)**: $$ RMSE = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2} $$\n - **MSE (Mean Squared Error)**: $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2 $$\n - **Rˆ2**: Coeficient of determination.\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DB027F574B3045E48CF73A20DAE62802"
    },
    "cell_type" : "code",
    "source" : "def buildEvaluator(label: String, predictionCol: String): RegressionEvaluator = {\n  new RegressionEvaluator()\n    .setLabelCol(label)\n    .setPredictionCol(predictionCol)\n}\n\n\ndef evaluateMSE(df: DataFrame, label: String, predictionCol: String): Double = {\n  val evaluator = buildEvaluator(label, predictionCol)\n  evaluator.setMetricName(\"mse\")\n  evaluator.evaluate(df)\n}\n\ndef evaluateR2(df: DataFrame, label: String, predictionCol: String): Double = {\n  val evaluator = buildEvaluator(label, predictionCol)\n  evaluator.setMetricName(\"r2\")\n  evaluator.evaluate(df)\n}\n\ndef evaluateRMSE(df: DataFrame, label: String, predictionCol: String): Double = {\n  val evaluator = buildEvaluator(label, predictionCol)\n  evaluator.setMetricName(\"rmse\")\n  evaluator.evaluate(df)\n}\n\n\n// prints result of algorithm tested\ndef buildStatsMaps(carrier: Double, col: Column, label: String, df: DataFrame, predictionCol: String): Map[String, Any] = {\n  val calculateAcc = (exp: Double, predicted: Double) => {\n    val error = (exp - predicted) / exp\n    if (error > 0.2) 0 else 1\n  }\n\n  val calcAccuracyUDF = udf(calculateAcc)\n\n  val rmse = evaluateRMSE(df, label, \"prediction_log\")\n  val mse = evaluateMSE(df, label, \"prediction_log\")\n  val r2 = evaluateR2(df, label, \"prediction_log\")\n\n  val data = df.withColumn(\"result_column\", calcAccuracyUDF(col, df(predictionCol)))\n  val total = data.count.toDouble\n  // filter prediction that got right\n  val correct = data.filter(\"result_column = 1\").count.toDouble\n  val accuracy = (correct / total) * 100\n\n  Map(\"rmse\" -> rmse, \"mse\" -> mse, \"r2\" -> r2, \"accuracy\" -> accuracy)\n}\n",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "buildEvaluator: (label: String, predictionCol: String)org.apache.spark.ml.evaluation.RegressionEvaluator\nevaluateMSE: (df: org.apache.spark.sql.DataFrame, label: String, predictionCol: String)Double\nevaluateR2: (df: org.apache.spark.sql.DataFrame, label: String, predictionCol: String)Double\nevaluateRMSE: (df: org.apache.spark.sql.DataFrame, label: String, predictionCol: String)Double\nbuildStatsMaps: (carrier: Double, col: org.apache.spark.sql.Column, label: String, df: org.apache.spark.sql.DataFrame, predictionCol: String)Map[String,Any]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 13,
      "time" : "Took: 960 milliseconds, at 2017-8-15 19:8"
    } ]
  }, {
    "metadata" : {
      "id" : "7C7FEFBEBA3C4A8282EC0A0776905E9E"
    },
    "cell_type" : "markdown",
    "source" : "### Building, training the model and evaluating the model\n\nWe are finally ready to build and train our model!\n<img src=\"http://127.0.0.1:8080/images/step3.png\" width=\"80%\" />\n\nFirst, we need to split our data in test and training. We will use 10% of data to validate and 90% to train.\nAfter that, we will create the model itself, the DecisionTreeRegressor, with our label and features. We will build also a Pipeline with two stages: build the assembler vector with our data and the decision tree.\n\nThere is also some parameters which are related with the model itself. They are named: *hyperparameters*. In the decision tree model we have basically two hyperparameters:\n- **MaxDepth**: How deep must be the decision tree?\n- **MaxBin**: How many bins must be the decision tree?\n\n\n<img src=\"http://127.0.0.1:8080/images/tree-model.png\" width=\"30%\" height=\"15%\" />\n\nWe will need also the evaluator to decide which of the trained models is the best model for the problem. "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab987973994-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "40E5217E1DB248929E990C6CFA24DFEB"
    },
    "cell_type" : "code",
    "source" : "val trainedModels = carrierList.map { c =>\n  val label  = \"successful_charges_log\"\n  val features = \"features_success\"\n  val predictionColumn = \"successful_charges\"\n  val assembler = assemblerSucccesful\n  val data = dataWithLabels.filter(s\"carrier_id = $c\")\n  val Array(trainingData, testData) = data.randomSplit(Array(0.9, 0.1))\n\n  // Train a DecisionTree model.\n  val decisionTree = new DecisionTreeRegressor()\n      .setLabelCol(label)\n      .setFeaturesCol(features)\n      .setPredictionCol(\"prediction_log\")\n\n  val pipeline = new Pipeline().setStages(Array(assemblerSucccesful, decisionTree))\n\n  val paramGrid = new ParamGridBuilder()\n     .addGrid(decisionTree.maxDepth, Array(6, 7, 8))\n     .addGrid(decisionTree.maxBins, (15 to 32).toList)\n     .build()\n\n  // Select (prediction, true label) and compute test error.\n  val evaluator = new RegressionEvaluator()\n     .setLabelCol(label)\n     .setPredictionCol(\"prediction_log\")\n     .setMetricName(\"rmse\")\n\n\n   val trainValidationSplit = new TrainValidationSplit()\n     .setEstimator(pipeline)\n     .setEvaluator(evaluator)\n     .setEstimatorParamMaps(paramGrid)\n     .setTrainRatio(0.8)\n\n   //train a model\n  val model = trainValidationSplit.fit(trainingData)\n\n  //make predictions\n  val predictions = model.transform(testData)\n  val columnValue = s\"prediction_$predictionColumn\"\n\n  val predictionResult = predictions.withColumn(columnValue, exp($\"prediction_log\"))\n  val statsMap = buildStatsMaps(c, predictionResult(columnValue), label, predictionResult, columnValue)\n  \n\n  val bestModel = model.bestModel.asInstanceOf[PipelineModel].stages(1).asInstanceOf[DecisionTreeRegressionModel]\n  println(s\"maxDepth: ${bestModel.getMaxDepth}, maxBins: ${bestModel.getMaxBins}\")\n  (bestModel, c, statsMap)\n}\n",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "maxDepth: 8, maxBins: 32\ntrainedModels: List[(org.apache.spark.ml.regression.DecisionTreeRegressionModel, Int, Map[String,Any])] = List((DecisionTreeRegressionModel (uid=dtr_493d5100093b) of depth 8 with 151 nodes,1,Map(rmse -> 0.08326642420427305, mse -> 0.006933297399765949, r2 -> 0.9467328397549113, accuracy -> 100.0)))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 14,
      "time" : "Took: 26 seconds 975 milliseconds, at 2017-8-15 19:8"
    } ]
  }, {
    "metadata" : {
      "id" : "CE3A1F5987C44AE080578B994A94ACB1"
    },
    "cell_type" : "markdown",
    "source" : "## Evaluating our model\n\nWe can now evaluate our model, simply print the stats already computed:\n<img src=\"http://127.0.0.1:8080/images/step4.png\" width=\"80%\" />"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D52D2633061E45D58EA46DE69EE56984"
    },
    "cell_type" : "code",
    "source" : "trainedModels.foreach{ case (m, c, statsMap) => \n                      m.write.overwrite.save(ROOT + \"/trained-models-dataframe/success-c\" + c)\n                      println(statsMap)}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "Map(rmse -> 0.08326642420427305, mse -> 0.006933297399765949, r2 -> 0.9467328397549113, accuracy -> 100.0)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 15,
      "time" : "Took: 1 second 484 milliseconds, at 2017-8-15 19:8"
    } ]
  } ],
  "nbformat" : 4
}